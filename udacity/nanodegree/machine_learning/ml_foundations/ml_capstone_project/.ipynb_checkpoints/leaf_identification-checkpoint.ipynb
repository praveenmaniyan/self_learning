{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item referenced:\n",
    "\n",
    "    https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the needed libraries here\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "('Model output shape is: ', (None, 4, 4, 512))\n",
      "Found 509 images belonging to 32 classes.\n",
      "Found 128 images belonging to 32 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\Lib\\site-packages\\ipykernel_launcher.py:76: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Python27\\Lib\\site-packages\\ipykernel_launcher.py:76: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=<keras.pre..., steps_per_epoch=31, epochs=10, validation_steps=128)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 854s 28s/step - loss: 250.2155 - acc: 1.0000 - val_loss: 249.8305 - val_acc: 1.0000\n",
      "Epoch 2/10\n",
      "31/31 [==============================] - 835s 27s/step - loss: 248.4478 - acc: 1.0000 - val_loss: 249.8305 - val_acc: 1.0000\n",
      "Epoch 3/10\n",
      "31/31 [==============================] - 812s 26s/step - loss: 248.2593 - acc: 1.0000 - val_loss: 249.8305 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "31/31 [==============================] - 813s 26s/step - loss: 252.1775 - acc: 1.0000 - val_loss: 249.8305 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "31/31 [==============================] - 814s 26s/step - loss: 246.1695 - acc: 1.0000 - val_loss: 249.8305 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "31/31 [==============================] - 809s 26s/step - loss: 255.0624 - acc: 1.0000 - val_loss: 249.8305 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "31/31 [==============================] - 834s 27s/step - loss: 247.3996 - acc: 1.0000 - val_loss: 249.8305 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "31/31 [==============================] - 893s 29s/step - loss: 246.6315 - acc: 1.0000 - val_loss: 249.8305 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "31/31 [==============================] - 961s 31s/step - loss: 256.0644 - acc: 1.0000 - val_loss: 249.8305 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "31/31 [==============================] - 975s 31s/step - loss: 249.3257 - acc: 1.0000 - val_loss: 249.8305 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10dc4470>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path to the model weights files.\n",
    "weights_path = 'weights/vgg16_weights.h5'\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 496\n",
    "nb_validation_samples = 128\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "# build the VGG16 network\n",
    "vgg16_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "print('Model loaded.')\n",
    "print(\"Model output shape is: \", vgg16_model.output_shape)\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=vgg16_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "model = Sequential() #new model\n",
    "for layer in vgg16_model.layers: \n",
    "    model.add(layer)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "model.add(top_model)\n",
    "\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:25]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=nb_train_samples,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('vgg_leaf_identification_v0.1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
