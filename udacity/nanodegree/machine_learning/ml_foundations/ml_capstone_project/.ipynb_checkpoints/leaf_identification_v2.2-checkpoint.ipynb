{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-c8f5c31aaa67>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-c8f5c31aaa67>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Item referenced:\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Item referenced:\n",
    "            https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "            https://github.com/keras-team/keras/issues/3296\n",
    "            https://www.codesofinterest.com/2017/08/bottleneck-features-multi-class-classification-keras.html\n",
    "            https://stackoverflow.com/questions/47535596/how-do-i-get-the-true-labels-when-i-use-a-imagedatagenerator-flow-from-directory?rq=1\n",
    "            https://stackoverflow.com/questions/45806669/keras-how-to-use-predict-generator-with-imagedatagenerator\n",
    "            https://keras.io/preprocessing/image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improt the nececssary packages and libraries\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras import applications, optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.  \n",
    "img_width, img_height =  150, 150\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model_weights.h5'\n",
    "top_model_save_path = 'bottleneck_fc_model.h5'\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "\n",
    "# number of epochs to train top model  \n",
    "epochs = 10\n",
    "# batch size\n",
    "batch_size = 16\n",
    "# Hard coded sample sizes\n",
    "nb_train_samples = 496\n",
    "nb_validation_samples = 128\n",
    "n_folds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function to save the training data features and its labels with the Image Data generator\n",
    "def save_train_bottlebeck_features(p_train_data_dir, p_img_width, p_img_height, p_batch_size):\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')  \n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "   \n",
    "    generator = datagen.flow_from_directory(  \n",
    "     p_train_data_dir,  \n",
    "     target_size=(p_img_width, p_img_height),  \n",
    "     batch_size=p_batch_size,  \n",
    "     class_mode=None,  \n",
    "     shuffle=False)  \n",
    "\n",
    "    l_nb_train_samples = len(generator.filenames)  \n",
    "    l_num_classes = len(generator.class_indices)\n",
    "    print(\"Train num classes \", l_num_classes)\n",
    "\n",
    "    predict_size_train = int(math.ceil(l_nb_train_samples / float(p_batch_size)))\n",
    "    print(\"predict_size_train is \", predict_size_train)\n",
    "    \n",
    "    bottleneck_features_train = model.predict_generator(  \n",
    "     generator, predict_size_train)\n",
    "    \n",
    "    print(\"bottleneck_features_train shape is \", bottleneck_features_train.shape)\n",
    "\n",
    "    np.save('bottleneck_features_train.npy', bottleneck_features_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_validation_bottleneck_features(p_validation_data_dir, p_img_width, p_img_height, p_batch_size):\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')  \n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    \n",
    "    generator = datagen.flow_from_directory(  \n",
    "     p_validation_data_dir,  \n",
    "     target_size=(p_img_width, p_img_height),  \n",
    "     batch_size=p_batch_size,  \n",
    "     class_mode=None,  \n",
    "     shuffle=False)  \n",
    "\n",
    "    l_nb_validation_samples = len(generator.filenames)\n",
    "    l_num_classes = len(generator.class_indices)\n",
    "    print(\"Validation num classes \", l_num_classes)\n",
    "\n",
    "    predict_size_validation = int(math.ceil(l_nb_validation_samples / float(p_batch_size)))# The float is very important here,\n",
    "    # else the predict_generator produces smaller numpy array\n",
    "    print(\"predict_size_validation is \", predict_size_validation)\n",
    "\n",
    "    bottleneck_features_validation = model.predict_generator(  \n",
    "     generator, predict_size_validation)  \n",
    "    \n",
    "    print(\"bottleneck_features_validation shape is \", bottleneck_features_validation.shape)\n",
    "\n",
    "    np.save('bottleneck_features_validation.npy', bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(i_fold, train_data, train_labels, p_num_train_classes, validation_data, validation_labels, p_batch_size, p_epochs):\n",
    "    \n",
    "    l_num_classes = p_num_train_classes\n",
    "    model = None\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', \n",
    "                            input_shape=train_data.shape[1:]))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64, kernel_size=1, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=1))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(l_num_classes, activation='softmax'))\n",
    "    from keras.utils.vis_utils import plot_model\n",
    "    plot_model(model, to_file='vgg.png')\n",
    "\n",
    "    #model.summary() \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    #model.compile(optimizer='rmsprop',  \n",
    "    #          loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    #model.compile(loss='binary_crossentropy',\n",
    "    #          optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "    #          metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_data, train_labels,  \n",
    "          epochs=p_epochs,  \n",
    "          batch_size=p_batch_size,  \n",
    "          validation_data=(validation_data, validation_labels))  \n",
    "\n",
    "    model.save_weights(top_model_weights_path + str(i_fold))\n",
    "    model.save(top_model_save_path + str(i_fold))\n",
    "\n",
    "    (eval_loss, eval_accuracy) = model.evaluate(  \n",
    "     validation_data, validation_labels, batch_size=p_batch_size, verbose=1)\n",
    "\n",
    "    print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))  \n",
    "    print(\"[INFO] Loss: {}\".format(eval_loss))\n",
    "    \n",
    "    plt.figure(1)  \n",
    "\n",
    "    # summarize history for accuracy  \n",
    "\n",
    "    plt.subplot(211)  \n",
    "    plt.plot(history.history['acc'])  \n",
    "    plt.plot(history.history['val_acc'])  \n",
    "    plt.title('model accuracy')  \n",
    "    plt.ylabel('accuracy')  \n",
    "    plt.xlabel('epoch')  \n",
    "    plt.legend(['train', 'test'], loc='upper left')  \n",
    "\n",
    "    # summarize history for loss  \n",
    "\n",
    "    plt.subplot(212)  \n",
    "    plt.plot(history.history['loss'])  \n",
    "    plt.plot(history.history['val_loss'])  \n",
    "    plt.title('model loss')  \n",
    "    plt.ylabel('loss')  \n",
    "    plt.xlabel('epoch')  \n",
    "    plt.legend(['train', 'test'], loc='upper left')  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_model(p_train_data_dir, p_validation_data_dir, p_img_width, p_img_height, p_batch_size, p_epochs):\n",
    "    datagen_top = ImageDataGenerator(rescale=1./255)  \n",
    "    generator_top = datagen_top.flow_from_directory(  \n",
    "         p_train_data_dir,  \n",
    "         target_size=(p_img_width, p_img_height),  \n",
    "         batch_size=p_batch_size,  \n",
    "         class_mode='categorical',  \n",
    "         shuffle=False)  \n",
    "\n",
    "    l_nb_train_samples = len(generator_top.filenames)  \n",
    "    l_num_classes = len(generator_top.class_indices)  \n",
    "\n",
    "    # load the bottleneck features saved earlier  \n",
    "    train_data = np.load('bottleneck_features_train.npy')  \n",
    "\n",
    "    # get the class lebels for the training data, in the original order  \n",
    "    train_labels_b = generator_top.classes  \n",
    "    #print(train_labels_b)\n",
    "\n",
    "    # convert the training labels to categorical vectors  \n",
    "    train_labels = to_categorical(train_labels_b, num_classes=l_num_classes)\n",
    "    \n",
    "    generator_top = datagen_top.flow_from_directory(  \n",
    "         p_validation_data_dir,  \n",
    "         target_size=(p_img_width, p_img_height),  \n",
    "         batch_size=p_batch_size,  \n",
    "         class_mode=None,  \n",
    "         shuffle=False)  \n",
    "\n",
    "    l_nb_validation_samples = len(generator_top.filenames)  \n",
    "\n",
    "    validation_data = np.load('bottleneck_features_validation.npy')  \n",
    "\n",
    "    validation_labels = generator_top.classes  \n",
    "    validation_labels = to_categorical(validation_labels, num_classes=l_num_classes)\n",
    "    #print(train_labels.shape)\n",
    "    \n",
    "    skf = StratifiedKFold(train_labels_b, n_folds=n_folds, shuffle=True)\n",
    "\n",
    "    for i, (e_train, e_test) in enumerate(skf):\n",
    "            print \"Running Fold\", i+1, \"/\", n_folds\n",
    "            train_and_evaluate_model(i, train_data[e_train], train_labels[e_train], l_num_classes, train_data[e_test], train_labels[e_test], p_batch_size, p_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 509 images belonging to 32 classes.\n",
      "Found 128 images belonging to 32 classes.\n",
      "Running Fold 1 / 3\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c082633f05ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#save_train_bottlebeck_features(train_data_dir, img_width, img_height, batch_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#save_validation_bottleneck_features(validation_data_dir, img_width, img_height, batch_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_top_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-91f7ebe3bae1>\u001b[0m in \u001b[0;36mtrain_top_model\u001b[1;34m(p_train_data_dir, p_validation_data_dir, p_img_width, p_img_height, p_batch_size, p_epochs)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0me_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Running Fold\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mtrain_and_evaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_num_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-9cf056167807>\u001b[0m in \u001b[0;36mtrain_and_evaluate_model\u001b[1;34m(i_fold, train_data, train_labels, p_num_train_classes, validation_data, validation_labels, p_batch_size, p_epochs)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml_num_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'vgg.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m#model.summary()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\Lib\\site-packages\\keras\\utils\\vis_utils.pyc\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[1;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \"\"\"\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\Lib\\site-packages\\keras\\utils\\vis_utils.pyc\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\Lib\\site-packages\\keras\\utils\\vis_utils.pyc\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# pydot raises a generic Exception here,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# so no specific class can be caught.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         raise ImportError('Failed to import pydot. You must install pydot'\n\u001b[0m\u001b[0;32m     32\u001b[0m                           ' and graphviz for `pydotprint` to work.')\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work."
     ]
    }
   ],
   "source": [
    "#save_train_bottlebeck_features(train_data_dir, img_width, img_height, batch_size)\n",
    "#save_validation_bottleneck_features(validation_data_dir, img_width, img_height, batch_size)\n",
    "train_top_model(train_data_dir, validation_data_dir, img_width, img_height, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
